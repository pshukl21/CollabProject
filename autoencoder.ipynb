{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6b4f90",
   "metadata": {},
   "source": [
    "This notebook describes the use of an autoencoder to predict future NBA games. The idea is to use the training data as \"patterns\". Then, when it comes time to predict, we would feed the network a \"messy\" pattern that consists of the two teams that are playing and all the other inputs are randomized. Ideally, the autoencoder should be able to reconstruct the game stats from just the messy input involving the two teams playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84070a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b93785",
   "metadata": {},
   "source": [
    "# Building the Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2582a7e",
   "metadata": {},
   "source": [
    "### __1.__ Prepare training data\n",
    "1. We need to make sure all categorical varialbes are one-hot encoded.\n",
    "2. Then 'mask' values (replace with 0s?) in corrupted training\n",
    "3. Add normal/poisson noise to continuous variables (depending on scale)\n",
    "4. Drop unneeded variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd368346",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = pd.read_csv(\"df_games_output.csv\")\n",
    "df_games.drop(columns=['gameId','gameDate'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075fdad",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16347b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the mean for numerical columns\n",
    "for col in df_games.columns:\n",
    "    if df_games[col].isna().sum() > 0:  # Check if the column has missing values\n",
    "        if df_games[col].dtype in ['float64', 'int64']:  # Only for numerical columns\n",
    "            df_games[col].fillna(df_games[col].median(), inplace=True)\n",
    "        if df_games[col].dtype == 'object':\n",
    "            df_games[col].fillna('Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759ea376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should show NO columns after imputation\n",
    "for col in df_games.columns:\n",
    "    if df_games[col].isna().sum() > 0:  # Check if the column has missing values\n",
    "        print(f\"Column {col} still has missing values after imputation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2a66f",
   "metadata": {},
   "source": [
    "Turning `winner` into a PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f78cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating team to index mapping for use with home team, away team, and winner one-hot encoded feaures\n",
    "unique_teams = df_games['hometeamName'].unique()\n",
    "team_to_index = {team: idx for idx, team in enumerate(unique_teams)}\n",
    "labels_winner = torch.tensor(df_games['winner'].map(team_to_index).values)\n",
    "one_hot_winner = F.one_hot(labels_winner,num_classes=len(unique_teams)).float()\n",
    "# 33 features for 33 unique teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4407a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_features = [\"winner_\" + team for team in df_games['hometeamName'].unique()]\n",
    "# Features named \"winner_TeamName\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5132e",
   "metadata": {},
   "source": [
    "Turning `hometeamName` into a PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9650d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50581, 33])\n"
     ]
    }
   ],
   "source": [
    "unique_teams = df_games['hometeamName'].unique()\n",
    "team_to_index = {team: idx for idx, team in enumerate(unique_teams)}\n",
    "labels_hometeam = torch.tensor(df_games['hometeamName'].map(team_to_index).values)\n",
    "one_hot_hometeam = F.one_hot(labels_hometeam, num_classes=len(unique_teams)).float()\n",
    "# [50581, 33]\n",
    "print(one_hot_hometeam.shape)\n",
    "# tensor has a column for each team (index 0 to 32) and the 0th index of the column is 1 if that row is the first team (Jazz)\n",
    "# 33 home teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c17f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "hometeam_features = ['hometeam_' + key for key in team_to_index.keys()]\n",
    "# Features are named hometeam_TeamName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596b68a",
   "metadata": {},
   "source": [
    "Turning `awayteamName` into a PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f708fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_awayteam = torch.tensor(df_games['awayteamName'].map(team_to_index).values)\n",
    "one_hot_awayteam = F.one_hot(labels_awayteam, num_classes=len(unique_teams)).float()\n",
    "# 33 away teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17047f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "awayteam_features = ['awayteam_' + key for key in team_to_index.keys()]\n",
    "# features are named \"awayteam_TeamName\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d1c7a",
   "metadata": {},
   "source": [
    "Turning `gameType` into a PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cd070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_gametypes = df_games['gameType'].unique()\n",
    "type_to_index = {gametype: idx for idx, gametype in enumerate(unique_gametypes)}\n",
    "labels_gametype= torch.tensor(df_games['gameType'].map(type_to_index).values)\n",
    "one_hot_gametype = F.one_hot(labels_gametype, num_classes=len(unique_gametypes)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "795b709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gametype_features = ['gametype_' + key for key in type_to_index.keys()]\n",
    "# 6 game types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561832b",
   "metadata": {},
   "source": [
    "Turning `df_games` into tensor for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbac9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaling the numerical columns\n",
    "for col in df_games.columns:\n",
    "    if df_games[col].dtype in ['float64', 'int64']:\n",
    "        df_games[[col]] = scaler.fit_transform(df_games[[col]])\n",
    "\n",
    "# creating tensor\n",
    "tensor_games = torch.tensor(df_games.drop(columns=['hometeamName','awayteamName','winner','gameType']).values)\n",
    "tensor_numeric = torch.cat([tensor_games,one_hot_hometeam, one_hot_awayteam,one_hot_winner, one_hot_gametype], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "234291f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All feature names\n",
    "numeric_features = df_games.drop(columns=['hometeamName','awayteamName','winner','gameType']).columns.tolist()\n",
    "tensor_features = numeric_features +  hometeam_features + awayteam_features + winner_features + gametype_features\n",
    "tensor_features = [feat.replace(\" \",\"_\") for feat in tensor_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12689515",
   "metadata": {},
   "source": [
    "### __2.__ Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2f7af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static network class\n",
    "\n",
    "class ae(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # specifies to run parent torch.nn.Module class __init__ method automatically when I initialize the child 'dae' class I'm making.\n",
    "        # Encoding layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(145,24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24,8)\n",
    "        )\n",
    "        # Decoding layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(8,24),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(24,24), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 145)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0225f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible NN class\n",
    "class fae(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, hidden_dim,compression_dim):\n",
    "        super().__init__() # specifies to run parent torch.nn.Module class __init__ method automatically when I initialize the child 'dae' class I'm making.\n",
    "\n",
    "        # Encoding layers\n",
    "        enc_lay = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for i in range(hidden_layers - 1):\n",
    "            enc_lay.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            enc_lay.append(nn.ReLU())\n",
    "        enc_lay.append(nn.Linear(hidden_dim, compression_dim))\n",
    "        self.encoder = nn.Sequential(*enc_lay)\n",
    "\n",
    "        # Decoding layers\n",
    "        output_dim = input_dim\n",
    "        dec_lay = [nn.Linear(compression_dim, hidden_dim), nn.ReLU()]\n",
    "        for i in range(hidden_layers - 1):\n",
    "            dec_lay.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            dec_lay.append(nn.ReLU())\n",
    "        dec_lay.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.decoder = nn.Sequential(*dec_lay)\n",
    "\n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8596877",
   "metadata": {},
   "source": [
    "### __3.__ Instantiate model\n",
    "* Instantiate loss function for combo of continuous and one-hot encoded variables\n",
    "> 1. We need to use the proper loss function for one-hot encoded features (binary cross-entropy; BCE).\n",
    "> 2. Then, specify a composite loss function that includes both binary feature loss (BCE) and continiuous feature loss (MSE).\n",
    "* Specify gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cfc2a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(prediction, pattern):\n",
    "    # numeric feature loss\n",
    "    num_loss_fn = nn.MSELoss()\n",
    "    num_loss = num_loss_fn(prediction[:,:40], pattern[:,:40])\n",
    "    \n",
    "    # CrossEntropy loss to auto apply softmax ans cross entropy loss to continuous outputs that should be treated as logits\n",
    "    oh_loss_fn = nn.CrossEntropyLoss()\n",
    "    ht_loss = oh_loss_fn(prediction[:, 40:(40+33)], torch.argmax(pattern[:, 40:(40+33)],dim=1))\n",
    "    at_loss = oh_loss_fn(prediction[:, 73:(73+33)], torch.argmax(pattern[:, 73:(73+33)],dim = 1))\n",
    "    win_loss = oh_loss_fn(prediction[:, 106:(106+33)], torch.argmax(pattern[:, 106:(106+33)],dim=1))\n",
    "    gt_loss = oh_loss_fn(prediction[:, 139:145], torch.argmax(pattern[:, 139:145],dim=1))\n",
    "\n",
    "    # weights\n",
    "    weights = torch.tensor([40 / 145,33 / 145,33 / 145,33 / 145,6 / 145])\n",
    "    losses = torch.stack([num_loss,ht_loss,at_loss,win_loss,gt_loss])\n",
    "    loss = torch.dot(weights, losses)\n",
    "    return loss\n",
    "\n",
    "# AdaM (Adaptive Moment estimation) is a pretty fancy off the shelf algorithm. It involves tracking recent gradient values to dynamically control the learning rate, which aids convergence\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 1e-1,\n",
    "                             weight_decay=1e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5d35e",
   "metadata": {},
   "source": [
    "### __4.__ Set up training regimen\n",
    "\n",
    "* Alter batch size\n",
    "* Alter network structure\n",
    "* Alter loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e7ad267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 42  # You can use any integer\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "model1 = ae()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model2 = fae(input_dim=145, hidden_layers=2, hidden_dim=24, compression_dim=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab423b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 5, Batch 750    [2.74603271484375, 2.7496278285980225, 2.7607407569885254, 2.7975330352783203, 2.7420668601989746, 2.754002571105957, 2.7259185314178467, 2.755215883255005, 2.7375264167785645, 2.737128496170044]\n",
      "Epoch 5 of 5, Batch 750    [2.74603271484375, 2.7496278285980225, 2.7607407569885254, 2.7975330352783203, 2.7420668601989746, 2.754002571105957, 2.7259185314178467, 2.755215883255005, 2.7375264167785645, 2.737128496170044]\n"
     ]
    }
   ],
   "source": [
    "# TESTING TO MAKE SURE FELXIBLE AUTOENCODER WORKS\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "loader = DataLoader(tensor_numeric, batch_size=64, shuffle=True, generator=g)\n",
    "# number of training epochs\n",
    "epochs = 5\n",
    "output = []\n",
    "losses1 = []\n",
    "\n",
    "counter = 0\n",
    "for epoch in range(1,epochs+1):\n",
    "    for i, batch in enumerate(loader):\n",
    "        if (i % 50) == 0: \n",
    "            print(f\"\\rEpoch {epoch} of {epochs}, Batch {i}    \",end='')\n",
    "        reconstructed = model1(batch.float())\n",
    "        # print(f\"\\nRECONSTRUCTED: {reconstructed}\\nRECONSTRUCTED SHAPE: {reconstructed.shape}\")\n",
    "        loss = loss_function(reconstructed, batch.float())\n",
    "        # print(f\"Batch {i} Loss: {loss.item()}\")\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # store losses\n",
    "        losses1.append(loss.item())\n",
    "        # store output\n",
    "        counter += 1\n",
    "        output.append((epoch,loss,counter))\n",
    "\n",
    "print(losses1[:10])\n",
    "\n",
    "\n",
    "# testing felxible autoencoder\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "loader = DataLoader(tensor_numeric, batch_size=64, shuffle=True, generator=g)\n",
    "# number of training epochs\n",
    "epochs = 5\n",
    "output = []\n",
    "losses2 = []\n",
    "\n",
    "counter = 0\n",
    "for epoch in range(1,epochs+1):\n",
    "    for i, batch in enumerate(loader):\n",
    "        if (i % 50) == 0: \n",
    "            print(f\"\\rEpoch {epoch} of {epochs}, Batch {i}    \",end='')\n",
    "        reconstructed = model2(batch.float())\n",
    "        # print(f\"\\nRECONSTRUCTED: {reconstructed}\\nRECONSTRUCTED SHAPE: {reconstructed.shape}\")\n",
    "        loss = loss_function(reconstructed, batch.float())\n",
    "        # print(f\"Batch {i} Loss: {loss.item()}\")\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # store losses\n",
    "        losses2.append(loss.item())\n",
    "        # store output\n",
    "        counter += 1\n",
    "        output.append((epoch,loss,counter))\n",
    "\n",
    "print(losses2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8032de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=16, compression_dim=4, batch_size=128: 2.732\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=16, compression_dim=8, batch_size=128: 2.754\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=16, compression_dim=16, batch_size=128: 2.731\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=24, compression_dim=4, batch_size=128: 2.716\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=24, compression_dim=8, batch_size=128: 2.748\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=24, compression_dim=16, batch_size=128: 2.745\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=32, compression_dim=4, batch_size=128: 2.754\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=32, compression_dim=8, batch_size=128: 2.724\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=1, hidden_dim=32, compression_dim=16, batch_size=128: 2.734\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=16, compression_dim=4, batch_size=128: 2.76\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=16, compression_dim=8, batch_size=128: 2.711\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=16, compression_dim=16, batch_size=128: 2.74\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=24, compression_dim=4, batch_size=128: 2.719\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=24, compression_dim=8, batch_size=128: 2.734\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=24, compression_dim=16, batch_size=128: 2.706\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=32, compression_dim=4, batch_size=128: 2.725\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=32, compression_dim=8, batch_size=128: 2.745\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=2, hidden_dim=32, compression_dim=16, batch_size=128: 2.731\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=16, compression_dim=4, batch_size=128: 2.72\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=16, compression_dim=8, batch_size=128: 2.722\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=16, compression_dim=16, batch_size=128: 2.76\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=24, compression_dim=4, batch_size=128: 2.745\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=24, compression_dim=8, batch_size=128: 2.735\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=24, compression_dim=16, batch_size=128: 2.732\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=32, compression_dim=4, batch_size=128: 2.744\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=32, compression_dim=8, batch_size=128: 2.739\n",
      "Epoch 5 of 5, Batch 350    \n",
      "Final Loss for layers=3, hidden_dim=32, compression_dim=16, batch_size=128: 2.717\n"
     ]
    }
   ],
   "source": [
    "# hyperparamater options\n",
    "hidden_layers_options = [1, 2, 3]\n",
    "hidden_dim_options = [16, 24, 32]\n",
    "compression_dim_options = [4, 8, 16]\n",
    "batch_size_options = [128]\n",
    "\n",
    "# Create the grid\n",
    "param_grid = list(itertools.product(\n",
    "    hidden_layers_options,\n",
    "    hidden_dim_options,\n",
    "    compression_dim_options,\n",
    "    batch_size_options\n",
    "))\n",
    "\n",
    "\n",
    "for hidden_layers, hidden_dim, compression_dim, batch_size in param_grid:\n",
    "\n",
    "    # Set seeds for reproducibility if needed\n",
    "    torch.manual_seed(42)\n",
    "    # Instantiate model\n",
    "    model = fae(input_dim=145, hidden_layers=hidden_layers, hidden_dim=hidden_dim, compression_dim=compression_dim)\n",
    "    # Set up DataLoader\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(42)\n",
    "    loader = DataLoader(tensor_numeric, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    # number of training epochs\n",
    "    epochs = 5\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for i, batch in enumerate(loader):\n",
    "            if (i % 50) == 0: \n",
    "                print(f\"\\rEpoch {epoch} of {epochs}, Batch {i}    \",end='')\n",
    "            reconstructed = model(batch.float())\n",
    "            # print(f\"\\nRECONSTRUCTED: {reconstructed}\\nRECONSTRUCTED SHAPE: {reconstructed.shape}\")\n",
    "            loss = loss_function(reconstructed, batch.float())\n",
    "            # print(f\"Batch {i} Loss: {loss.item()}\")\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # store losses\n",
    "            losses.append(loss.item())\n",
    "    print(f\"\\nFinal Loss for layers={hidden_layers}, hidden_dim={hidden_dim}, compression_dim={compression_dim}, batch_size={batch_size}: {round(losses[-1],3)}\")\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
